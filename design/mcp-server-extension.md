# High-Level Design Plan: MCP-SCRT Server Extension

## Executive Summary

This design extends your existing MCP-SCRT server to incorporate **ChromaDB** (vector knowledge base), **Neo4j** (graph database for relationship tracking), and **Redis** (intelligent caching layer). The design maintains the proven architecture of your 637-test production-ready MCP server while adding sophisticated AI-powered capabilities.

---

## 1. Architecture Overview

### Current State
```
┌─────────────────────────────────────────────┐
│         Gradio Application (UI)             │
└─────────────────────────────────────────────┘
                    ↓
┌─────────────────────────────────────────────┐
│         MCP-SCRT Server (60 Tools)          │
│  • Network, Wallet, Bank, Blockchain        │
│  • Staking, Governance, Contracts, IBC      │
└─────────────────────────────────────────────┘
                    ↓
┌─────────────────────────────────────────────┐
│       Secret Network Blockchain             │
└─────────────────────────────────────────────┘
```

### Target State
```
┌─────────────────────────────────────────────┐
│         Gradio Application (UI)             │
│  • Chat Interface                           │
│  • Portfolio Dashboard                      │
│  • Validator Explorer                       │
│  • Settings & Wallet Management             │
└─────────────────────────────────────────────┘
                    ↓
┌─────────────────────────────────────────────┐
│         Agent Orchestration Layer           │
│  • LLM Router (Ollama: Llama3.3:70b)        │
│  • Intent Classifier                        │
│  • Multi-Step Planner                       │
│  • Response Formatter                       │
└─────────────────────────────────────────────┘
        ↓                ↓                ↓
┌──────────────┐  ┌──────────────┐  ┌──────────────┐
│   ChromaDB   │  │   Neo4j      │  │   Redis      │
│   Vector KB  │  │   Graph DB   │  │   Cache      │
└──────────────┘  └──────────────┘  └──────────────┘
                    ↓
┌─────────────────────────────────────────────┐
│         Enhanced MCP-SCRT Server            │
│  • 60 Existing Blockchain Tools             │
│  • +15 New Knowledge Tools                  │
│  • +12 New Graph Analysis Tools             │
│  • +8 New Caching Tools                     │
└─────────────────────────────────────────────┘
                    ↓
┌─────────────────────────────────────────────┐
│       Secret Network Blockchain             │
└─────────────────────────────────────────────┘
```

---

## 2. Database Integration Strategy

### 2.1 ChromaDB Integration (Vector Knowledge Base)

**Purpose**: Semantic search and retrieval of Secret Network documentation, guides, and contextual knowledge.

**New MCP Tools (7 tools)**:
1. **`knowledge_search`** - Semantic search across knowledge base
2. **`knowledge_add_document`** - Add new documentation
3. **`knowledge_update_document`** - Update existing documentation
4. **`knowledge_delete_document`** - Remove outdated content
5. **`knowledge_list_collections`** - List knowledge categories
6. **`knowledge_get_similar`** - Find related content
7. **`knowledge_rebuild_index`** - Rebuild vector embeddings

**Data Structure**:
```python
# Knowledge Chunk Schema
{
    "id": "fundamentals_001",
    "collection": "fundamentals",  # Topic category
    "title": "What is Secret Network?",
    "content": "Secret Network is a blockchain...",
    "metadata": {
        "category": "fundamentals",
        "tags": ["blockchain", "privacy", "TEE"],
        "version": "2.0",
        "last_updated": "2025-11-14",
        "source_file": "fundamentals.md",
        "chunk_index": 0,
        "total_chunks": 15
    },
    "embedding": [0.123, -0.456, ...]  # Generated by ChromaDB
}
```

**Collections Structure**:
```
chromadb/
├── fundamentals/      # What is Secret Network, Architecture
├── privacy_tech/      # TEE, SGX, Encryption Mechanisms
├── tokens/            # SCRT, SNIP-20, Wrapping, Keys
├── staking/           # Delegation, Validators, Rewards
├── contracts/         # Smart Contracts, CosmWasm
├── security/          # Best Practices, Wallet Safety
└── faq/              # Common Questions & Answers
```

**Embedding Strategy**:
- **Model**: `sentence-transformers/all-MiniLM-L6-v2` (384 dimensions)
- **Chunk Size**: 500-1000 tokens with 100 token overlap
- **Distance Metric**: Cosine similarity
- **Retrieval**: Top-K (K=5) with MMR re-ranking

**Integration Points**:
```python
# In mcp-scrt/tools/knowledge/
class KnowledgeSearchTool(BaseTool):
    """Search knowledge base using semantic similarity."""
    
    async def execute(
        self,
        query: str,
        collection: Optional[str] = None,
        top_k: int = 5,
        min_similarity: float = 0.6
    ) -> List[KnowledgeResult]:
        """
        Query knowledge base and return relevant documents.
        
        Args:
            query: Natural language query
            collection: Optional collection filter
            top_k: Number of results to return
            min_similarity: Minimum cosine similarity threshold
            
        Returns:
            List of relevant knowledge chunks with metadata
        """
```

---

### 2.2 Neo4j Integration (Graph Database)

**Purpose**: Track relationships between blockchain entities (wallets, validators, contracts, proposals) and enable graph-based analysis.

**New MCP Tools (12 tools)**:

**Graph Management (4 tools)**:
1. **`graph_create_node`** - Create entity node (Wallet, Validator, Contract, Proposal)
2. **`graph_create_relationship`** - Create relationship between nodes
3. **`graph_delete_node`** - Remove node and relationships
4. **`graph_update_node`** - Update node properties

**Graph Query (5 tools)**:
5. **`graph_find_path`** - Find shortest path between entities
6. **`graph_get_neighbors`** - Get connected entities
7. **`graph_analyze_centrality`** - Identify important nodes
8. **`graph_detect_communities`** - Find clusters of related entities
9. **`graph_query_cypher`** - Execute custom Cypher query

**Analytics (3 tools)**:
10. **`graph_validator_network`** - Analyze validator delegation patterns
11. **`graph_contract_interactions`** - Map contract call relationships
12. **`graph_proposal_voting`** - Analyze voting patterns

**Graph Schema**:
```cypher
// Node Types
(:Wallet {address, balance, created_at, last_active})
(:Validator {address, moniker, commission, voting_power, uptime})
(:Contract {address, code_id, label, creator, instantiated_at})
(:Proposal {id, title, status, submit_time, voting_end})
(:Transaction {hash, timestamp, type, amount, gas_used})

// Relationship Types
(:Wallet)-[:DELEGATES {amount, timestamp}]->(:Validator)
(:Wallet)-[:SENT {amount, timestamp}]->(:Wallet)
(:Wallet)-[:EXECUTED {timestamp, success}]->(:Contract)
(:Wallet)-[:VOTED {option, timestamp}]->(:Proposal)
(:Contract)-[:CALLS {method, timestamp}]->(:Contract)
(:Validator)-[:REDELEGATED_FROM {amount, timestamp}]->(:Validator)
```

**Example Queries**:
```cypher
// Find validators that a wallet's delegates are redelegating to
MATCH (w:Wallet {address: $address})-[:DELEGATES]->(v1:Validator)
      -[:REDELEGATED_FROM]->(v2:Validator)
RETURN v2.moniker, COUNT(*) as delegation_count
ORDER BY delegation_count DESC
LIMIT 5

// Identify influential wallets in governance
MATCH (w:Wallet)-[v:VOTED]->(p:Proposal)
WITH w, COUNT(p) as votes_cast,
     SIZE((w)-[:DELEGATES]->()) as validators_delegated
WHERE votes_cast > 10
RETURN w.address, votes_cast, validators_delegated
ORDER BY votes_cast DESC
LIMIT 20

// Map contract interaction patterns
MATCH path = (c1:Contract)-[:CALLS*1..3]->(c2:Contract)
WHERE c1.label CONTAINS 'DEX'
RETURN path
LIMIT 100
```

**Integration Points**:
```python
# In mcp-scrt/tools/graph/
class GraphValidatorNetworkTool(BaseTool):
    """Analyze validator network and delegation patterns."""
    
    async def execute(
        self,
        validator_address: Optional[str] = None,
        depth: int = 2,
        min_delegation: float = 1000.0
    ) -> GraphAnalysisResult:
        """
        Analyze validator network structure.
        
        Args:
            validator_address: Optional validator to focus on
            depth: Graph traversal depth (1-3)
            min_delegation: Minimum delegation amount to include
            
        Returns:
            Network analysis with visualization data
        """
```

---

### 2.3 Redis Integration (Intelligent Caching)

**Purpose**: Cache frequently accessed data, reduce blockchain RPC calls, and improve response times.

**New MCP Tools (8 tools)**:

**Cache Management (4 tools)**:
1. **`cache_set`** - Store data with TTL
2. **`cache_get`** - Retrieve cached data
3. **`cache_delete`** - Remove cache entry
4. **`cache_clear_pattern`** - Clear multiple keys by pattern

**Cache Analytics (4 tools)**:
5. **`cache_get_stats`** - Get cache hit/miss statistics
6. **`cache_get_keys`** - List cached keys by pattern
7. **`cache_get_memory_usage`** - Check cache memory usage
8. **`cache_optimize`** - Run cache optimization (LRU cleanup)

**Caching Strategy**:

```python
# Cache Key Patterns
CACHE_PATTERNS = {
    # Short TTL (30 seconds) - Rapidly changing data
    "balance:{address}": 30,
    "gas_price": 30,
    "block:latest": 30,
    
    # Medium TTL (5 minutes) - Moderately changing data
    "validator:{address}": 300,
    "delegations:{address}": 300,
    "rewards:{address}": 300,
    "account:{address}": 300,
    
    # Long TTL (1 hour) - Slowly changing data
    "validators:all": 3600,
    "proposals:all": 3600,
    "code_info:{code_id}": 3600,
    
    # Very Long TTL (24 hours) - Static data
    "block:{height}": 86400,
    "tx:{hash}": 86400,
    "contract_info:{address}": 86400,
}
```

**Cache Layers**:
```
┌─────────────────────────────────────┐
│  L1: Request Cache (Redis)          │
│  • Raw RPC responses                │
│  • TTL: 30s - 24h                   │
└─────────────────────────────────────┘
            ↓ (miss)
┌─────────────────────────────────────┐
│  L2: Computed Cache (Redis)         │
│  • Aggregated data                  │
│  • Analysis results                 │
│  • TTL: 5m - 1h                     │
└─────────────────────────────────────┘
            ↓ (miss)
┌─────────────────────────────────────┐
│  L3: Knowledge Cache (Redis)        │
│  • ChromaDB query results           │
│  • LLM responses                    │
│  • TTL: 1h - 24h                    │
└─────────────────────────────────────┘
            ↓ (miss)
┌─────────────────────────────────────┐
│  Source: Blockchain RPC / Database  │
└─────────────────────────────────────┘
```

**Smart Cache Invalidation**:
```python
# Cache invalidation rules
INVALIDATION_RULES = {
    # When wallet sends transaction, invalidate:
    "tx:send": ["balance:{from}", "balance:{to}", "account:{from}"],
    
    # When delegation occurs, invalidate:
    "tx:delegate": [
        "balance:{delegator}",
        "delegations:{delegator}",
        "validator:{validator}",
        "validators:all"
    ],
    
    # When rewards withdrawn, invalidate:
    "tx:withdraw_rewards": [
        "balance:{delegator}",
        "rewards:{delegator}",
        "delegations:{delegator}"
    ],
    
    # When proposal created, invalidate:
    "tx:submit_proposal": ["proposals:all", "governance:stats"],
    
    # When vote cast, invalidate:
    "tx:vote": ["proposal:{id}", "governance:stats:{id}"]
}
```

**Integration Points**:
```python
# In mcp-scrt/tools/cache/
class CacheGetTool(BaseTool):
    """Retrieve data from cache with automatic fallback."""
    
    async def execute(
        self,
        key: str,
        fetch_on_miss: bool = True,
        ttl_extend: Optional[int] = None
    ) -> CacheResult:
        """
        Get cached data with intelligent fallback.
        
        Args:
            key: Cache key
            fetch_on_miss: Auto-fetch from source on miss
            ttl_extend: Extend TTL on hit (seconds)
            
        Returns:
            Cached data or freshly fetched data
        """
```

---

## 3. MCP Server Extension Architecture

### 3.1 New Directory Structure

```
mcp-scrt/
├── src/mcp_scrt/
│   ├── tools/
│   │   ├── network/         # Existing (4 tools)
│   │   ├── wallet/          # Existing (6 tools)
│   │   ├── bank/            # Existing (5 tools)
│   │   ├── blockchain/      # Existing (5 tools)
│   │   ├── account/         # Existing (3 tools)
│   │   ├── transaction/     # Existing (5 tools)
│   │   ├── staking/         # Existing (8 tools)
│   │   ├── rewards/         # Existing (4 tools)
│   │   ├── governance/      # Existing (6 tools)
│   │   ├── contracts/       # Existing (10 tools)
│   │   ├── ibc/             # Existing (4 tools)
│   │   │
│   │   ├── knowledge/       # NEW (7 tools)
│   │   │   ├── __init__.py
│   │   │   ├── search.py
│   │   │   ├── add.py
│   │   │   ├── update.py
│   │   │   ├── delete.py
│   │   │   ├── collections.py
│   │   │   ├── similar.py
│   │   │   └── rebuild.py
│   │   │
│   │   ├── graph/           # NEW (12 tools)
│   │   │   ├── __init__.py
│   │   │   ├── nodes.py            # Create/update/delete nodes
│   │   │   ├── relationships.py    # Manage relationships
│   │   │   ├── query.py            # Path finding, neighbors
│   │   │   ├── analytics.py        # Centrality, communities
│   │   │   ├── validators.py       # Validator network analysis
│   │   │   ├── contracts.py        # Contract interactions
│   │   │   └── governance.py       # Voting patterns
│   │   │
│   │   └── cache/           # NEW (8 tools)
│   │       ├── __init__.py
│   │       ├── operations.py       # Set/get/delete
│   │       ├── patterns.py         # Pattern operations
│   │       ├── stats.py            # Analytics
│   │       └── optimize.py         # Optimization
│   │
│   ├── integrations/        # NEW
│   │   ├── __init__.py
│   │   ├── chromadb_client.py     # ChromaDB wrapper
│   │   ├── neo4j_client.py        # Neo4j driver wrapper
│   │   ├── redis_client.py        # Redis client wrapper
│   │   └── ollama_client.py       # Ollama LLM client
│   │
│   ├── services/            # NEW
│   │   ├── __init__.py
│   │   ├── knowledge_service.py   # Knowledge base logic
│   │   ├── graph_service.py       # Graph analysis logic
│   │   ├── cache_service.py       # Cache management logic
│   │   └── embedding_service.py   # Vector embedding generation
│   │
│   └── middleware/          # NEW
│       ├── __init__.py
│       ├── cache_middleware.py    # Automatic caching
│       ├── graph_middleware.py    # Auto graph updates
│       └── telemetry.py           # Performance monitoring
```

### 3.2 Tool Registration

```python
# In mcp-scrt/src/mcp_scrt/tools/__init__.py

# Existing tool categories
from .network import *
from .wallet import *
# ... (existing imports)

# NEW tool categories
from .knowledge import *
from .graph import *
from .cache import *

# Updated tool registry
ALL_TOOLS = {
    # Existing: 60 tools
    **NETWORK_TOOLS,
    **WALLET_TOOLS,
    **BANK_TOOLS,
    **BLOCKCHAIN_TOOLS,
    **ACCOUNT_TOOLS,
    **TRANSACTION_TOOLS,
    **STAKING_TOOLS,
    **REWARDS_TOOLS,
    **GOVERNANCE_TOOLS,
    **CONTRACTS_TOOLS,
    **IBC_TOOLS,
    
    # NEW: 27 tools
    **KNOWLEDGE_TOOLS,    # 7 tools
    **GRAPH_TOOLS,        # 12 tools
    **CACHE_TOOLS,        # 8 tools
}

# Total: 87 tools
```

---

## 4. Data Flow Patterns

### 4.1 Knowledge Retrieval Flow

```
User Query: "What is Secret Network?"
    ↓
[1] Gradio UI receives query
    ↓
[2] Agent Orchestration Layer
    ├─→ Intent Classification (LLM)
    └─→ Classifies as: "information_request"
    ↓
[3] Route to Knowledge System
    ├─→ Check Redis cache: "knowledge:query:{hash(query)}"
    │   ├─→ HIT: Return cached response (0.5ms)
    │   └─→ MISS: Continue
    ↓
[4] ChromaDB Vector Search
    ├─→ Generate query embedding (50ms)
    ├─→ Semantic search: top_k=5, min_similarity=0.6
    └─→ Returns: 5 relevant chunks
    ↓
[5] LLM Synthesis (Ollama)
    ├─→ System prompt with retrieved context
    ├─→ Generate natural response (2-3s)
    └─→ Include citations to source chunks
    ↓
[6] Cache result in Redis
    ├─→ Key: "knowledge:query:{hash(query)}"
    └─→ TTL: 1 hour
    ↓
[7] Return to Gradio UI
    └─→ Display with markdown formatting
```

### 4.2 Blockchain Operation Flow with Auto-Graphing

```
User Action: "Stake 100 SCRT to validator X"
    ↓
[1] Gradio UI receives command
    ↓
[2] Agent Orchestration Layer
    ├─→ Intent Classification: "transaction"
    ├─→ Entity Extraction: amount=100, validator=X
    └─→ Multi-step Planner creates workflow
    ↓
[3] Pre-Transaction Checks
    ├─→ Cache check: "balance:{address}" (Redis)
    ├─→ Cache check: "validator:{validator_addr}" (Redis)
    ├─→ Risk assessment (sufficient balance, valid validator)
    └─→ User confirmation
    ↓
[4] Execute Delegation (Existing MCP Tool)
    ├─→ Call: secret_delegate(validator, amount)
    ├─→ Transaction broadcast
    └─→ Returns: tx_hash
    ↓
[5] Cache Invalidation (Middleware)
    ├─→ Delete: "balance:{address}"
    ├─→ Delete: "delegations:{address}"
    ├─→ Delete: "validator:{validator}"
    └─→ Delete: "validators:all"
    ↓
[6] Graph Update (Middleware)
    ├─→ Neo4j: CREATE/UPDATE (:Wallet {address})
    ├─→ Neo4j: CREATE/UPDATE (:Validator {address})
    ├─→ Neo4j: CREATE (:Wallet)-[:DELEGATES {amount, timestamp}]->(:Validator)
    └─→ Async operation (non-blocking)
    ↓
[7] Return Success
    ├─→ Transaction hash
    ├─→ Confirmation message
    └─→ Graph update status
```

### 4.3 Hybrid Query Flow (Knowledge + Blockchain)

```
User Query: "Explain staking and show my current stakes"
    ↓
[1] Agent Orchestration Layer
    ├─→ Intent Classification: "hybrid" (info + query)
    └─→ Splits into two sub-tasks
    ↓
[2] Parallel Execution
    ├─→ Task A: Knowledge Retrieval
    │   ├─→ ChromaDB: Search "staking" topic
    │   ├─→ LLM: Synthesize explanation
    │   └─→ Returns: Educational content
    │
    └─→ Task B: Blockchain Query
        ├─→ Cache check: "delegations:{address}" (Redis)
        ├─→ MCP Tool: secret_get_delegations()
        └─→ Returns: Active delegations
    ↓
[3] LLM Response Formatting
    ├─→ Combine: Knowledge + Data
    ├─→ Structure:
    │   ├─→ Section 1: Staking Explanation
    │   ├─→ Section 2: Your Current Stakes (table)
    │   └─→ Section 3: Recommendations (if applicable)
    └─→ Format as markdown
    ↓
[4] Return to UI
    └─→ Render formatted response
```

---

## 5. Middleware & Automation

### 5.1 Cache Middleware

**Purpose**: Automatically cache all blockchain RPC responses and invalidate on state changes.

```python
# In mcp-scrt/middleware/cache_middleware.py

class CacheMiddleware:
    """Automatic caching for blockchain operations."""
    
    def __init__(self, redis_client):
        self.redis = redis_client
        self.patterns = CACHE_PATTERNS
        self.invalidation = INVALIDATION_RULES
    
    async def before_execute(self, tool_name: str, params: dict):
        """Check cache before tool execution."""
        cache_key = self._generate_key(tool_name, params)
        
        if cached := await self.redis.get(cache_key):
            return CacheHit(data=cached, key=cache_key)
        
        return CacheMiss()
    
    async def after_execute(
        self,
        tool_name: str,
        params: dict,
        result: Any
    ):
        """Cache result after tool execution."""
        cache_key = self._generate_key(tool_name, params)
        ttl = self._get_ttl(tool_name)
        
        await self.redis.set(cache_key, result, ex=ttl)
        
        # If transaction, invalidate related caches
        if tool_name.startswith("secret_send") or \
           tool_name.startswith("secret_delegate"):
            await self._invalidate_related(tool_name, params)
```

### 5.2 Graph Middleware

**Purpose**: Automatically update Neo4j graph when blockchain operations occur.

```python
# In mcp-scrt/middleware/graph_middleware.py

class GraphMiddleware:
    """Automatic graph updates for blockchain operations."""
    
    def __init__(self, neo4j_driver):
        self.neo4j = neo4j_driver
    
    async def after_execute(
        self,
        tool_name: str,
        params: dict,
        result: Any
    ):
        """Update graph after successful operation."""
        
        # Delegation operations
        if tool_name == "secret_delegate":
            await self._record_delegation(
                delegator=params["address"],
                validator=params["validator_address"],
                amount=params["amount"],
                tx_hash=result["tx_hash"]
            )
        
        # Transfer operations
        elif tool_name == "secret_send_tokens":
            await self._record_transfer(
                from_addr=params["address"],
                to_addr=params["recipient"],
                amount=params["amount"],
                tx_hash=result["tx_hash"]
            )
        
        # Governance operations
        elif tool_name == "secret_vote_proposal":
            await self._record_vote(
                voter=params["address"],
                proposal_id=params["proposal_id"],
                option=params["vote_option"],
                tx_hash=result["tx_hash"]
            )
        
        # Contract operations
        elif tool_name == "secret_execute_contract":
            await self._record_contract_execution(
                executor=params["address"],
                contract=params["contract_address"],
                msg=params["execute_msg"],
                tx_hash=result["tx_hash"]
            )
```

### 5.3 Telemetry Middleware

**Purpose**: Track performance metrics and tool usage statistics.

```python
# In mcp-scrt/middleware/telemetry.py

class TelemetryMiddleware:
    """Performance monitoring and analytics."""
    
    async def before_execute(self, tool_name: str, params: dict):
        """Record start time and request."""
        return {
            "tool": tool_name,
            "start_time": time.time(),
            "params_size": len(str(params))
        }
    
    async def after_execute(
        self,
        tool_name: str,
        context: dict,
        result: Any,
        error: Optional[Exception] = None
    ):
        """Record metrics."""
        duration = time.time() - context["start_time"]
        
        metrics = {
            "tool": tool_name,
            "duration_ms": duration * 1000,
            "success": error is None,
            "result_size": len(str(result)),
            "timestamp": datetime.utcnow()
        }
        
        # Store in Redis for analytics
        await self.redis.lpush("metrics:tools", json.dumps(metrics))
        await self.redis.ltrim("metrics:tools", 0, 9999)  # Keep last 10k
        
        # Update tool usage counters
        await self.redis.hincrby("stats:tool_usage", tool_name, 1)
```

---

## 6. Service Layer Architecture

### 6.1 Knowledge Service

```python
# In mcp-scrt/services/knowledge_service.py

class KnowledgeService:
    """High-level knowledge base operations."""
    
    def __init__(
        self,
        chroma_client,
        redis_client,
        ollama_client,
        embedding_service
    ):
        self.chroma = chroma_client
        self.redis = redis_client
        self.ollama = ollama_client
        self.embeddings = embedding_service
    
    async def search(
        self,
        query: str,
        collection: Optional[str] = None,
        top_k: int = 5
    ) -> KnowledgeSearchResult:
        """
        Semantic search with caching and LLM synthesis.
        """
        # 1. Check cache
        cache_key = f"knowledge:search:{hash(query)}"
        if cached := await self.redis.get(cache_key):
            return KnowledgeSearchResult.parse(cached)
        
        # 2. Generate embedding
        query_embedding = await self.embeddings.embed(query)
        
        # 3. Search ChromaDB
        collections = [collection] if collection else self._all_collections
        results = []
        
        for coll in collections:
            docs = await self.chroma.query(
                collection_name=coll,
                query_embeddings=[query_embedding],
                n_results=top_k
            )
            results.extend(docs)
        
        # 4. Re-rank results
        ranked = await self._rerank(query, results, top_k)
        
        # 5. LLM synthesis
        synthesized = await self._synthesize_response(query, ranked)
        
        # 6. Cache result
        result = KnowledgeSearchResult(
            query=query,
            results=ranked,
            synthesized=synthesized
        )
        await self.redis.set(cache_key, result.json(), ex=3600)
        
        return result
    
    async def _synthesize_response(
        self,
        query: str,
        documents: List[Document]
    ) -> str:
        """Use LLM to synthesize natural response."""
        context = "\n\n".join([
            f"[Source: {doc.metadata['title']}]\n{doc.content}"
            for doc in documents
        ])
        
        prompt = f"""Using the following context, answer the question naturally.
        Include citations to the sources.

        Context:
        {context}

        Question: {query}

        Answer:"""
        
        response = await self.ollama.generate(
            model="llama3.3:70b",
            prompt=prompt,
            stream=False
        )
        
        return response["response"]
```

### 6.2 Graph Service

```python
# In mcp-scrt/services/graph_service.py

class GraphService:
    """High-level graph database operations."""
    
    def __init__(self, neo4j_driver, redis_client):
        self.neo4j = neo4j_driver
        self.redis = redis_client
    
    async def analyze_validator_network(
        self,
        validator_address: Optional[str] = None,
        depth: int = 2
    ) -> ValidatorNetworkAnalysis:
        """
        Analyze validator delegation patterns and relationships.
        """
        # Check cache
        cache_key = f"graph:validator_network:{validator_address}:{depth}"
        if cached := await self.redis.get(cache_key):
            return ValidatorNetworkAnalysis.parse(cached)
        
        # Execute Cypher query
        async with self.neo4j.session() as session:
            if validator_address:
                # Focused analysis on one validator
                query = """
                MATCH path = (w:Wallet)-[:DELEGATES*1..%d]->(v:Validator)
                WHERE v.address = $validator_address
                RETURN path
                """ % depth
                params = {"validator_address": validator_address}
            else:
                # Global network analysis
                query = """
                MATCH (v:Validator)
                OPTIONAL MATCH (v)<-[d:DELEGATES]-(w:Wallet)
                WITH v, COUNT(w) as delegator_count, SUM(d.amount) as total_delegated
                RETURN v, delegator_count, total_delegated
                ORDER BY total_delegated DESC
                LIMIT 50
                """
                params = {}
            
            result = await session.run(query, params)
            records = await result.data()
        
        # Process results
        analysis = self._process_validator_network(records)
        
        # Cache for 5 minutes
        await self.redis.set(cache_key, analysis.json(), ex=300)
        
        return analysis
    
    async def record_transaction(
        self,
        tx_type: str,
        tx_hash: str,
        from_addr: str,
        to_addr: Optional[str] = None,
        amount: Optional[str] = None,
        **metadata
    ):
        """Record transaction in graph database."""
        async with self.neo4j.session() as session:
            # Create/update wallet nodes
            await session.run("""
                MERGE (from:Wallet {address: $from_addr})
                ON CREATE SET from.created_at = timestamp()
                SET from.last_active = timestamp()
                """, {"from_addr": from_addr})
            
            if to_addr:
                await session.run("""
                    MERGE (to:Wallet {address: $to_addr})
                    ON CREATE SET to.created_at = timestamp()
                    """, {"to_addr": to_addr})
            
            # Create transaction node and relationships
            if tx_type == "delegate":
                await session.run("""
                    MATCH (w:Wallet {address: $from_addr})
                    MERGE (v:Validator {address: $to_addr})
                    CREATE (w)-[:DELEGATES {
                        amount: $amount,
                        timestamp: timestamp(),
                        tx_hash: $tx_hash
                    }]->(v)
                    """, {
                        "from_addr": from_addr,
                        "to_addr": to_addr,
                        "amount": amount,
                        "tx_hash": tx_hash
                    })
            
            elif tx_type == "send":
                await session.run("""
                    MATCH (from:Wallet {address: $from_addr})
                    MATCH (to:Wallet {address: $to_addr})
                    CREATE (from)-[:SENT {
                        amount: $amount,
                        timestamp: timestamp(),
                        tx_hash: $tx_hash
                    }]->(to)
                    """, {
                        "from_addr": from_addr,
                        "to_addr": to_addr,
                        "amount": amount,
                        "tx_hash": tx_hash
                    })
```

### 6.3 Cache Service

```python
# In mcp-scrt/services/cache_service.py

class CacheService:
    """High-level caching operations with smart invalidation."""
    
    def __init__(self, redis_client):
        self.redis = redis_client
        self.patterns = CACHE_PATTERNS
        self.invalidation = INVALIDATION_RULES
    
    async def get_or_fetch(
        self,
        key: str,
        fetch_fn: Callable,
        ttl: Optional[int] = None
    ) -> Any:
        """
        Get from cache or fetch from source.
        """
        # Try cache first
        if cached := await self.redis.get(key):
            await self._record_hit(key)
            return json.loads(cached)
        
        # Cache miss - fetch from source
        await self._record_miss(key)
        data = await fetch_fn()
        
        # Store in cache
        ttl = ttl or self._get_ttl_for_key(key)
        await self.redis.set(key, json.dumps(data), ex=ttl)
        
        return data
    
    async def invalidate_related(
        self,
        operation: str,
        params: dict
    ):
        """
        Invalidate all related cache keys based on operation.
        """
        if operation not in self.invalidation:
            return
        
        patterns = self.invalidation[operation]
        keys_to_delete = []
        
        for pattern in patterns:
            # Substitute parameters
            key = pattern.format(**params)
            keys_to_delete.append(key)
        
        if keys_to_delete:
            await self.redis.delete(*keys_to_delete)
            await self._record_invalidation(operation, len(keys_to_delete))
    
    async def get_statistics(self) -> CacheStatistics:
        """Get cache performance statistics."""
        hits = int(await self.redis.get("cache:stats:hits") or 0)
        misses = int(await self.redis.get("cache:stats:misses") or 0)
        total = hits + misses
        hit_rate = (hits / total * 100) if total > 0 else 0
        
        memory_info = await self.redis.info("memory")
        
        return CacheStatistics(
            hits=hits,
            misses=misses,
            hit_rate=hit_rate,
            memory_used=memory_info["used_memory_human"],
            total_keys=await self.redis.dbsize()
        )
```

---

## 7. Integration with Gradio Application

### 7.1 Agent Orchestration Layer

The Gradio app communicates with the MCP server through an **Agent Orchestration Layer** that:

1. **Routes requests** to appropriate systems (Knowledge, Blockchain, or both)
2. **Coordinates multi-step workflows**
3. **Handles LLM interactions** with Ollama
4. **Formats responses** for the UI

```python
# In secret-agent/src/agent/orchestrator.py

class AgentOrchestrator:
    """Main agent that routes requests and coordinates execution."""
    
    def __init__(
        self,
        mcp_bridge,          # Bridge to MCP-SCRT server
        ollama_client,       # Ollama LLM client
        knowledge_service,   # ChromaDB knowledge base
        graph_service,       # Neo4j graph queries
        cache_service        # Redis caching
    ):
        self.mcp = mcp_bridge
        self.llm = ollama_client
        self.knowledge = knowledge_service
        self.graph = graph_service
        self.cache = cache_service
    
    async def process_message(
        self,
        message: str,
        context: ConversationContext
    ) -> AgentResponse:
        """Process user message and generate response."""
        
        # 1. Classify intent
        intent = await self._classify_intent(message, context)
        
        # 2. Route based on intent
        if intent.type == "information":
            response = await self._handle_knowledge_query(message, intent)
        
        elif intent.type == "transaction":
            response = await self._handle_blockchain_operation(message, intent)
        
        elif intent.type == "query":
            response = await self._handle_blockchain_query(message, intent)
        
        elif intent.type == "analysis":
            response = await self._handle_graph_analysis(message, intent)
        
        elif intent.type == "hybrid":
            response = await self._handle_hybrid_request(message, intent)
        
        else:
            response = await self._handle_general_conversation(message)
        
        # 3. Update conversation context
        context.add_turn(message, response)
        
        return response
    
    async def _handle_knowledge_query(
        self,
        message: str,
        intent: Intent
    ) -> AgentResponse:
        """Handle information requests using knowledge base."""
        
        # Search knowledge base
        kb_results = await self.knowledge.search(
            query=message,
            collection=intent.topic,
            top_k=5
        )
        
        return AgentResponse(
            type="knowledge",
            content=kb_results.synthesized,
            sources=kb_results.results,
            confidence=kb_results.confidence
        )
    
    async def _handle_blockchain_operation(
        self,
        message: str,
        intent: Intent
    ) -> AgentResponse:
        """Handle blockchain transactions."""
        
        # 1. Extract entities and parameters
        entities = await self._extract_entities(message, intent)
        
        # 2. Plan execution
        plan = await self._create_execution_plan(intent, entities)
        
        # 3. Risk assessment
        risk = await self._assess_risk(plan)
        
        # 4. Execute with user confirmation
        if risk.requires_confirmation:
            # Return confirmation request to UI
            return AgentResponse(
                type="confirmation",
                content=f"Confirm: {plan.description}?",
                plan=plan,
                risk_level=risk.level
            )
        
        # 5. Execute plan
        result = await self._execute_plan(plan)
        
        # 6. Update graph automatically (via middleware)
        
        return AgentResponse(
            type="transaction",
            content=self._format_transaction_result(result),
            tx_hash=result.tx_hash,
            status="success"
        )
    
    async def _handle_graph_analysis(
        self,
        message: str,
        intent: Intent
    ) -> AgentResponse:
        """Handle graph database queries."""
        
        # Analyze graph patterns
        if "validator" in message.lower():
            analysis = await self.graph.analyze_validator_network()
        elif "contract" in message.lower():
            analysis = await self.graph.analyze_contract_network()
        else:
            analysis = await self.graph.custom_query(message)
        
        # Format visualization data
        viz_data = self._format_graph_visualization(analysis)
        
        return AgentResponse(
            type="graph_analysis",
            content=analysis.summary,
            visualization=viz_data,
            insights=analysis.insights
        )
```

### 7.2 Gradio UI Integration

The Gradio application connects to the Agent Orchestrator:

```python
# In secret-agent/src/ui/app.py (simplified)

import gradio as gr
from agent.orchestrator import AgentOrchestrator

# Initialize orchestrator
orchestrator = AgentOrchestrator(...)

def chat_interface(message, history):
    """Handle chat messages."""
    
    # Create context from history
    context = ConversationContext(history=history)
    
    # Process message
    response = await orchestrator.process_message(message, context)
    
    # Format based on response type
    if response.type == "knowledge":
        return format_knowledge_response(response)
    
    elif response.type == "transaction":
        return format_transaction_response(response)
    
    elif response.type == "graph_analysis":
        return format_graph_response(response)
    
    elif response.type == "confirmation":
        return format_confirmation_request(response)

# Gradio interface
with gr.Blocks() as demo:
    chatbot = gr.Chatbot()
    msg_box = gr.Textbox()
    
    msg_box.submit(
        chat_interface,
        inputs=[msg_box, chatbot],
        outputs=[chatbot]
    )
```

---

## 8. Key Design Decisions & Rationale

### 8.1 Why Three Separate Databases?

**Decision**: Use ChromaDB, Neo4j, and Redis as separate, specialized systems.

**Rationale**:
- **ChromaDB**: Optimized for vector similarity search (semantic knowledge retrieval)
- **Neo4j**: Native graph database (relationship queries, path finding, network analysis)
- **Redis**: In-memory key-value store (low-latency caching, high throughput)

**Alternative Considered**: Using a single database (PostgreSQL with extensions).
**Rejected Because**: No single database can match the performance of specialized systems for these distinct workloads.

### 8.2 Why Middleware Pattern?

**Decision**: Use middleware for automatic caching and graph updates.

**Rationale**:
- **Separation of Concerns**: Business logic stays clean, cross-cutting concerns are handled separately
- **Non-Invasive**: Existing 60 tools don't need modification
- **Testable**: Middleware can be tested independently
- **Optional**: Can be enabled/disabled without changing core functionality

**Alternative Considered**: Manually adding cache logic to each tool.
**Rejected Because**: Would require modifying 60 existing tools and make code harder to maintain.

### 8.3 Why Service Layer?

**Decision**: Introduce a service layer between tools and database clients.

**Rationale**:
- **Abstraction**: Tools don't need to know database implementation details
- **Reusability**: Complex operations (like LLM synthesis) can be reused
- **Testing**: Services can be mocked for testing tools
- **Flexibility**: Can swap database implementations without changing tools

### 8.4 Why Async/Await?

**Decision**: Use async/await throughout the new code.

**Rationale**:
- **Performance**: Non-blocking I/O for database and RPC calls
- **Scalability**: Handle multiple concurrent requests efficiently
- **Consistency**: Match modern Python best practices
- **Gradio Compatibility**: Gradio 6 supports async event handlers

### 8.5 Why Automatic Graph Updates via Middleware?

**Decision**: Automatically update Neo4j graph when blockchain operations occur.

**Rationale**:
- **Zero Friction**: Users don't need to explicitly call graph update tools
- **Consistency**: Graph always reflects blockchain state
- **Real-time**: Graph updates happen immediately after transactions
- **Analysis-Ready**: Graph data available for instant analysis

**Alternative Considered**: Requiring users to manually call graph update tools.
**Rejected Because**: Too much cognitive overhead, users would forget, graph would become stale.

---

## 9. Extension Points & Future Enhancements

This design provides clear extension points for future enhancements:

### 9.1 Additional Tool Categories

```python
# Easy to add new tool categories

# In mcp-scrt/tools/defi/
class DEXSwapTool(BaseTool):
    """Execute token swaps on SecretSwap."""
    async def execute(self, ...): ...

# In mcp-scrt/tools/nft/
class NFTMintTool(BaseTool):
    """Mint NFTs on Stashh."""
    async def execute(self, ...): ...

# Just register in __init__.py
from .defi import *
from .nft import *

ALL_TOOLS = {
    **EXISTING_TOOLS,
    **DEFI_TOOLS,
    **NFT_TOOLS
}
```

### 9.2 Additional Knowledge Collections

```python
# Easy to add new knowledge topics
chromadb/
├── fundamentals/
├── privacy_tech/
├── tokens/
├── staking/
├── contracts/
├── security/
├── faq/
├── defi/           # NEW
├── nfts/           # NEW
└── governance/     # NEW (split from FAQ)
```

### 9.3 Additional Graph Analyses

```python
# Easy to add new graph analysis tools
class GraphDEFIPathsTool(BaseTool):
    """Analyze DeFi liquidity paths."""
    async def execute(self, ...): ...

class GraphWhaleWatcherTool(BaseTool):
    """Identify large token holders and movements."""
    async def execute(self, ...): ...
```

### 9.4 Advanced Caching Strategies

```python
# Can add more sophisticated caching
class PredictiveCacheService:
    """Predictively cache based on usage patterns."""
    
    async def prefetch_likely_queries(self, user_context):
        """Predict and cache likely next queries."""
        ...
```

---

## 10. Testing Strategy

### 10.1 Unit Tests for New Components

```python
# tests/test_knowledge_service.py
async def test_knowledge_search():
    service = KnowledgeService(mock_chroma, mock_redis, mock_ollama)
    result = await service.search("What is Secret Network?")
    assert len(result.results) > 0
    assert result.synthesized is not None

# tests/test_graph_service.py
async def test_validator_network_analysis():
    service = GraphService(mock_neo4j, mock_redis)
    result = await service.analyze_validator_network()
    assert len(result.validators) > 0

# tests/test_cache_service.py
async def test_cache_get_or_fetch():
    service = CacheService(mock_redis)
    data = await service.get_or_fetch("test:key", lambda: {"value": 123})
    assert data["value"] == 123
```

### 10.2 Integration Tests

```python
# tests/integration/test_mcp_tools.py
async def test_knowledge_search_tool_integration():
    """Test KnowledgeSearchTool with real ChromaDB."""
    tool = KnowledgeSearchTool(context)
    result = await tool.execute(query="Secret Network")
    assert result.ok
    assert len(result.results) > 0

async def test_graph_validator_network_tool_integration():
    """Test GraphValidatorNetworkTool with real Neo4j."""
    tool = GraphValidatorNetworkTool(context)
    result = await tool.execute()
    assert result.ok
    assert len(result.validators) > 0
```

### 10.3 End-to-End Tests

```python
# tests/e2e/test_agent_flows.py
async def test_knowledge_to_action_flow():
    """Test: User learns about staking, then stakes tokens."""
    
    # Step 1: Ask about staking
    response1 = await orchestrator.process_message(
        "What is staking?",
        context
    )
    assert response1.type == "knowledge"
    assert "delegate" in response1.content.lower()
    
    # Step 2: Perform staking
    response2 = await orchestrator.process_message(
        "Stake 100 SCRT to best validator",
        context
    )
    assert response2.type == "transaction"
    assert response2.tx_hash is not None
    
    # Step 3: Verify graph was updated
    graph_data = await graph_service.get_delegations(user_address)
    assert len(graph_data.delegations) > 0
```

---

## 11. Migration Path & Rollout

### Phase 1: Core Infrastructure (Week 1)
- Set up ChromaDB, Neo4j, Redis
- Create client wrappers
- Implement service layer
- Write unit tests

### Phase 2: Knowledge Base (Week 1-2)
- Create markdown content (7 topics)
- Implement knowledge tools (7 tools)
- Test semantic search
- Integrate with Gradio UI

### Phase 3: Caching Layer (Week 2)
- Implement cache tools (8 tools)
- Add cache middleware
- Test performance improvements
- Monitor cache hit rates

### Phase 4: Graph Database (Week 2-3)
- Implement graph tools (12 tools)
- Add graph middleware
- Create graph analyses
- Build visualizations

### Phase 5: Integration & Polish (Week 3)
- Connect all components
- End-to-end testing
- Performance optimization
- Documentation

---

## 12. Success Metrics

### Performance Targets
- **Knowledge Search**: <100ms (cached), <2s (uncached with LLM)
- **Blockchain Query**: <50ms (cached), <500ms (uncached)
- **Graph Query**: <200ms (simple), <2s (complex)
- **Cache Hit Rate**: >70% for steady-state usage
- **Overall Response Time**: <3s for 95th percentile

### Functionality Targets
- **Total MCP Tools**: 87 (60 existing + 27 new)
- **Knowledge Coverage**: 7 topics, ~5000 words, 100% accuracy
- **Graph Entities**: Track all user wallets, validators, contracts
- **Cache Keys**: Support 20+ key patterns with smart TTLs

### User Experience Targets
- **Conversation Quality**: Natural, contextual, cited responses
- **Action Success Rate**: >95% for valid operations
- **Error Handling**: Clear, actionable error messages
- **Mobile Compatibility**: Full functionality on mobile devices

---

## Summary

This high-level design extends your production-ready MCP-SCRT server (60 tools, 637 tests) with:

**27 New MCP Tools**:
- 7 Knowledge tools (ChromaDB)
- 12 Graph tools (Neo4j)
- 8 Cache tools (Redis)

**Key Architecture Patterns**:
- Service layer for business logic
- Middleware for cross-cutting concerns
- Client wrappers for database access
- Agent orchestration for Gradio integration

**Key Benefits**:
- **Educational**: Semantic knowledge search with LLM synthesis
- **Analytical**: Graph-based relationship analysis
- **Performant**: Intelligent multi-layer caching
- **Maintainable**: Clean separation of concerns
- **Extensible**: Clear extension points for future features

**Implementation Complexity**: Medium
- Leverages existing 22.5k LOC foundation
- Adds ~5k LOC for new functionality
- Maintains architectural consistency
- Preserves all 637 existing tests

